{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required Google Cloud packages (commented out as these are typically one-time setup commands)\n",
        "# !pip install gcloud\n",
        "# !gcloud auth application-default login\n",
        "\n",
        "# Import necessary Python libraries\n",
        "import pandas as pd                # Data manipulation and analysis\n",
        "import numpy as np                 # Numerical computing\n",
        "import time                        # Time-related functions\n",
        "import os                          # Operating system interfaces\n",
        "import pandas_gbq                  # Pandas integration with BigQuery\n",
        "from google.cloud import bigquery  # BigQuery client library\n",
        "import glob                        # File path pattern matching\n",
        "import openpyxl                    # Excel file handling\n",
        "import csv                         # CSV file handling\n",
        "import re                          # Regular expressions\n",
        "\n",
        "# Note: The actual imports remain exactly as in the original code"
      ],
      "metadata": {
        "id": "vzkQQ3QTFUW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamento"
      ],
      "metadata": {
        "id": "maUl2LLbgg07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library, which is essential for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from a CSV file into a pandas DataFrame.\n",
        "df = pd.read_csv(\"ilostat_todos_paises_genero.csv\")\n",
        "\n",
        "# This line was commented out, it was likely intended to scale a value.\n",
        "#df['obs_value'] = df['obs_value'] * 1000\n",
        "\n",
        "# This line would display the DataFrame in an interactive environment like Jupyter.\n",
        "df\n",
        "\n",
        "# Select a subset of the original columns to keep for the analysis.\n",
        "# This filters the DataFrame to only include the specified columns.\n",
        "df = df[['ref_area.label', 'source.label', 'time', 'sex.label', 'classif2.label', 'obs_value']]\n",
        "\n",
        "# Rename the selected columns to more descriptive and user-friendly names in Portuguese.\n",
        "df = df.rename(columns={\n",
        "    'ref_area.label':'pais',\n",
        "    'source.label':'fonte_pesquisa',\n",
        "    'time':'ano_pesquisa',\n",
        "    'sex.label':'genero',\n",
        "    'classif2.label':'setor',\n",
        "    'obs_value':'quantidade'\n",
        "})\n",
        "\n",
        "# Clean the 'setor' column by removing the prefix 'Institutional sector: ' from its values.\n",
        "df['setor'] = df['setor'].str.replace('Institutional sector: ', '')\n",
        "\n",
        "# Define a list of specific countries to be included in the final analysis.\n",
        "paises_selecionados = ['South Africa', 'Argentina', 'Bolivia (Plurinational State of)', 'Brazil', 'Chile', 'Colombia', 'United States of America', 'France', 'Mexico', 'Peru', 'Uruguay']\n",
        "\n",
        "# Step 1: Find the most recent year of data available for each country.\n",
        "# This is done by grouping by 'pais' and finding the maximum 'ano_pesquisa'.\n",
        "ano_max_por_pais = df.groupby('pais')['ano_pesquisa'].max().reset_index()\n",
        "\n",
        "# Step 2: Filter the result from the previous step to include only the countries\n",
        "# that are present in the 'paises_selecionados' list.\n",
        "ano_max_selecionados = ano_max_por_pais[ano_max_por_pais['pais'].isin(paises_selecionados)]\n",
        "\n",
        "# Step 3: Merge the original DataFrame with the filtered maximum year data.\n",
        "# The 'inner' join ensures that only rows for the selected countries and their most recent year are kept.\n",
        "df_final = pd.merge(\n",
        "    df,\n",
        "    ano_max_selecionados,\n",
        "    on=['pais', 'ano_pesquisa'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Step 4: Further filter the DataFrame to include only the 'Private' and 'Public' sectors.\n",
        "# After filtering, the index is reset to be sequential.\n",
        "df_final = df_final[df_final['setor'].isin(['Private', 'Public'])].reset_index(drop=True)\n",
        "\n",
        "# Create a dictionary to map the original country names to their Portuguese translations.\n",
        "mapeamento_paises = {\n",
        "    'Argentina': 'Argentina',\n",
        "    'Bolivia (Plurinational State of)': 'Bolívia',\n",
        "    'Brazil': 'Brasil',\n",
        "    'Chile': 'Chile',\n",
        "    'Colombia': 'Colômbia',\n",
        "    'France': 'França',\n",
        "    'Mexico': 'México',\n",
        "    'Peru': 'Peru',\n",
        "    'Uruguay': 'Uruguai',\n",
        "    'United States of America': 'Estados Unidos',\n",
        "    'South Africa': 'África do Sul'\n",
        "}\n",
        "\n",
        "# Apply the mapping to the 'pais' column to translate the country names.\n",
        "df_final['pais'] = df_final['pais'].replace(mapeamento_paises)\n",
        "\n",
        "# Scale the 'quantidade' column by multiplying its values by 1000.\n",
        "# This is often done when the original unit is in thousands.\n",
        "df_final['quantidade'] = df_final['quantidade'] * 1000\n",
        "\n",
        "# Translate the values in the 'genero' column from English to Portuguese.\n",
        "df_final['genero'] = df_final['genero'].replace({\n",
        "    'Female': 'Mulher',\n",
        "    'Male': 'Homem'\n",
        "})\n",
        "\n",
        "# Translate the values in the 'setor' column from English to Portuguese.\n",
        "df_final['setor'] = df_final['setor'].replace({\n",
        "    'Public': 'Público',\n",
        "    'Private': 'Privado'\n",
        "})\n",
        "\n",
        "# Convert the data type of the 'quantidade' column to integer.\n",
        "df_final['quantidade'] = df_final['quantidade'].astype(int)"
      ],
      "metadata": {
        "id": "VLBoJyZJFV7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlr7JARuzhT1",
        "outputId": "e9f25e3e-c335-4406-b26f-b4cbb410a1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44 entries, 0 to 43\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   pais            44 non-null     object\n",
            " 1   fonte_pesquisa  44 non-null     object\n",
            " 2   ano_pesquisa    44 non-null     int64 \n",
            " 3   genero          44 non-null     object\n",
            " 4   setor           44 non-null     object\n",
            " 5   quantidade      44 non-null     int64 \n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 2.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload"
      ],
      "metadata": {
        "id": "DDnAgrgZnvbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the BigQuery table.\n",
        "# This specifies each column's name, data type (e.g., STRING, INTEGER), and a description.\n",
        "schema=[\n",
        "    bigquery.SchemaField('pais','STRING',description='Nome do país selecionado'),\n",
        "    bigquery.SchemaField('fonte_pesquisa','STRING',description='De qual pesquisa nacional foi extraído aquele dado.'),\n",
        "    bigquery.SchemaField('genero','STRING',description='Gênero da pessoa'),\n",
        "    bigquery.SchemaField('quantidade','INTEGER',description='total de pessoas que trabalham'),\n",
        "    bigquery.SchemaField('setor','STRING',description='se o setor é público ou privado'),\n",
        "    bigquery.SchemaField('ano_pesquisa','INTEGER',description='Ano de coleta da informação'),\n",
        "]\n",
        "\n",
        "# A comment in Portuguese indicating the purpose of this section: \"Uploading to datalake\".\n",
        "## Subindo para datalake\n",
        "\n",
        "# Initialize the BigQuery client, specifying the Google Cloud project ID to connect to.\n",
        "client = bigquery.Client(project='repositoriodedadosgpsp')\n",
        "\n",
        "# Create a reference to the target BigQuery dataset named 'perfil_remuneracao'.\n",
        "dataset_ref = client.dataset('perfil_remuneracao')\n",
        "\n",
        "# Create a reference to the target table within the dataset.\n",
        "# The table will be named 'ILOSTAT_paises_selecionados_genero_v4'.\n",
        "table_ref = dataset_ref.table('ILOSTAT_paises_selecionados_genero_v4')\n",
        "\n",
        "# Configure the load job. By passing the 'schema' object, we ensure the table is created\n",
        "# with the correct data types and column descriptions.\n",
        "job_config = bigquery.LoadJobConfig(schema=schema)\n",
        "\n",
        "# Start the load job to upload the pandas DataFrame (df_final) to the specified BigQuery table.\n",
        "# This sends the data from your local script to Google Cloud.\n",
        "job = client.load_table_from_dataframe(df_final, table_ref, job_config=job_config)\n",
        "\n",
        "# Wait for the load job to complete. This is a blocking call that will pause the script\n",
        "# until the upload is finished or fails, raising an exception on error.\n",
        "job.result()"
      ],
      "metadata": {
        "id": "jEtj1I0KGbjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}